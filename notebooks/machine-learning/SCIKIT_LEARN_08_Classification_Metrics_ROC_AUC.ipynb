{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "SCIKIT_LEARN_08_Classification_Metrics_ROC_AUC.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dlmacedo/maxtrack/blob/master/notebooks/machine-learning/SCIKIT_LEARN_08_Classification_Metrics_ROC_AUC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVP4mX7zbOm5",
        "colab_type": "text"
      },
      "source": [
        "# Evaluating a classification model: ROC Curves and Area Under the Curve (AUC)\n",
        "\n",
        "Created by [Data School](http://www.dataschool.io/). Modified by [David MacÃªdo](https://github.com/dlmacedo). **Note:** This notebook uses Python 3.6 and scikit-learn 0.19.1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2cJpnpzbOm8",
        "colab_type": "text"
      },
      "source": [
        "## Agenda\n",
        "\n",
        "- How can you adjust classifier performance by **changing the classification threshold**?\n",
        "- What is the purpose of an **ROC curve**?\n",
        "- How does **Area Under the Curve (AUC)** differ from classification accuracy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpGwVpmLbOm9",
        "colab_type": "text"
      },
      "source": [
        "## Review of model evaluation\n",
        "\n",
        "- Need a way to choose between models: different model types, tuning parameters, and features\n",
        "- Use a **model evaluation procedure** to estimate how well a model will generalize to out-of-sample data\n",
        "- Requires a **model evaluation metric** to quantify the model performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2ppIyM4bOm-",
        "colab_type": "text"
      },
      "source": [
        "### Model evaluation procedures\n",
        "\n",
        "1. **Training and testing on the same data**\n",
        "    - Rewards overly complex models that \"overfit\" the training data and won't necessarily generalize\n",
        "2. **Train/test split**\n",
        "    - Split the dataset into two pieces, so that the model can be trained and tested on different data\n",
        "    - Better estimate of out-of-sample performance, but still a \"high variance\" estimate\n",
        "    - Useful due to its speed, simplicity, and flexibility\n",
        "3. **K-fold cross-validation**\n",
        "    - Systematically create \"K\" train/test splits and average the results together\n",
        "    - Even better estimate of out-of-sample performance\n",
        "    - Runs \"K\" times slower than train/test split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMfEyM2cbOn4",
        "colab_type": "text"
      },
      "source": [
        "## Adjusting the classification threshold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YRMMSWobOn5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print the first 10 predicted responses\n",
        "logreg.predict(X_test)[0:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7eSd0labOn7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print the first 10 predicted probabilities of class membership\n",
        "logreg.predict_proba(X_test)[0:10, :]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTzCdNvYbOn9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print the first 10 predicted probabilities for class 1\n",
        "logreg.predict_proba(X_test)[0:10, 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PvF3EbZbOn_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# store the predicted probabilities for class 1\n",
        "y_pred_prob = logreg.predict_proba(X_test)[:, 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQH5Q4O9bOoB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# allow plots to appear in the notebook\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIzY1Ac9bOoC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# histogram of predicted probabilities\n",
        "plt.hist(y_pred_prob, bins=8)\n",
        "plt.xlim(0, 1)\n",
        "plt.title('Histogram of predicted probabilities')\n",
        "plt.xlabel('Predicted probability of diabetes')\n",
        "plt.ylabel('Frequency')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmRv43rqbOoE",
        "colab_type": "text"
      },
      "source": [
        "**Decrease the threshold** for predicting diabetes in order to **increase the sensitivity** of the classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNVykvd2bOoE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# predict diabetes if the predicted probability is greater than 0.3\n",
        "from sklearn.preprocessing import binarize\n",
        "y_pred_class = binarize([y_pred_prob], 0.3)[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpUUBUmKbOoG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print the first 10 predicted probabilities\n",
        "y_pred_prob[0:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0MSHc3ibOoH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print the first 10 predicted classes with the lower threshold\n",
        "y_pred_class[0:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0XPnu6MbOoJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# previous confusion matrix (default threshold of 0.5)\n",
        "print(confusion)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqPYY4eqbOoK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# new confusion matrix (threshold of 0.3)\n",
        "print(metrics.confusion_matrix(y_test, y_pred_class))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szY8Vd2YbOoM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sensitivity has increased (used to be 0.24)\n",
        "print(46 / float(46 + 16))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCoO43a0bOoN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# specificity has decreased (used to be 0.91)\n",
        "print(80 / float(80 + 50))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLmMdaAAbOoQ",
        "colab_type": "text"
      },
      "source": [
        "**Conclusion:**\n",
        "\n",
        "- **Threshold of 0.5** is used by default (for binary problems) to convert predicted probabilities into class predictions\n",
        "- Threshold can be **adjusted** to increase sensitivity or specificity\n",
        "- Sensitivity and specificity have an **inverse relationship**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ys6zzHBDbOoQ",
        "colab_type": "text"
      },
      "source": [
        "## ROC Curves and Area Under the Curve (AUC)\n",
        "\n",
        "**Question:** Wouldn't it be nice if we could see how sensitivity and specificity are affected by various thresholds, without actually changing the threshold?\n",
        "\n",
        "**Answer:** Plot the ROC curve!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fk2Lu7irbOoQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# IMPORTANT: first argument is true values, second argument is predicted probabilities\n",
        "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_prob)\n",
        "plt.plot(fpr, tpr)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.0])\n",
        "plt.title('ROC curve for diabetes classifier')\n",
        "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
        "plt.ylabel('True Positive Rate (Sensitivity)')\n",
        "plt.grid(True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQB4wPWWbOoT",
        "colab_type": "text"
      },
      "source": [
        "- ROC curve can help you to **choose a threshold** that balances sensitivity and specificity in a way that makes sense for your particular context\n",
        "- You can't actually **see the thresholds** used to generate the curve on the ROC curve itself"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a78l7Zn2bOoT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define a function that accepts a threshold and prints sensitivity and specificity\n",
        "def evaluate_threshold(threshold):\n",
        "    print('Sensitivity:', tpr[thresholds > threshold][-1])\n",
        "    print('Specificity:', 1 - fpr[thresholds > threshold][-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlBpzOldbOoV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluate_threshold(0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFlEUwlxbOoW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluate_threshold(0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lw8nrVXBbOoX",
        "colab_type": "text"
      },
      "source": [
        "AUC is the **percentage** of the ROC plot that is **underneath the curve**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYhaqUKPbOoY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# IMPORTANT: first argument is true values, second argument is predicted probabilities\n",
        "print(metrics.roc_auc_score(y_test, y_pred_prob))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TeewbglbOoa",
        "colab_type": "text"
      },
      "source": [
        "- AUC is useful as a **single number summary** of classifier performance.\n",
        "- If you randomly chose one positive and one negative observation, AUC represents the likelihood that your classifier will assign a **higher predicted probability** to the positive observation.\n",
        "- AUC is useful even when there is **high class imbalance** (unlike classification accuracy)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RFhfFjYbOob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate cross-validated AUC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "cross_val_score(logreg, X, y, cv=10, scoring='roc_auc').mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGKNMrBJbOoe",
        "colab_type": "text"
      },
      "source": [
        "**Confusion matrix advantages:**\n",
        "\n",
        "- Allows you to calculate a **variety of metrics**\n",
        "- Useful for **multi-class problems** (more than two response classes)\n",
        "\n",
        "**ROC/AUC advantages:**\n",
        "\n",
        "- Does not require you to **set a classification threshold**\n",
        "- Still useful when there is **high class imbalance**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a_nxH55bOoe",
        "colab_type": "text"
      },
      "source": [
        "## ROC and AUC Resources\n",
        "\n",
        "- Video: [ROC Curves and Area Under the Curve](https://www.youtube.com/watch?v=OAl6eAyP-yo) (14 minutes), including [transcript and screenshots](http://www.dataschool.io/roc-curves-and-auc-explained/) and a [visualization](http://www.navan.name/roc/)\n",
        "- Video: [ROC Curves](https://www.youtube.com/watch?v=21Igj5Pr6u4) (12 minutes) by Rahul Patwari\n",
        "- Paper: [An introduction to ROC analysis](http://people.inf.elte.hu/kiss/13dwhdm/roc.pdf) by Tom Fawcett\n",
        "- Usage examples: [Comparing different feature sets](http://research.microsoft.com/pubs/205472/aisec10-leontjeva.pdf) for detecting fraudulent Skype users, and [comparing different classifiers](http://www.cse.ust.hk/nevinZhangGroup/readings/yi/Bradley_PR97.pdf) on a number of popular datasets\n",
        "\n",
        "## Other Resources\n",
        "\n",
        "- scikit-learn documentation: [Model evaluation](http://scikit-learn.org/stable/modules/model_evaluation.html)\n",
        "- Guide: [Comparing model evaluation procedures and metrics](https://github.com/justmarkham/DAT8/blob/master/other/model_evaluation_comparison.md)\n",
        "- Video: [Counterfactual evaluation of machine learning models](https://www.youtube.com/watch?v=QWCSxAKR-h0) (45 minutes) about how Stripe evaluates its fraud detection model, including [slides](http://www.slideshare.net/MichaelManapat/counterfactual-evaluation-of-machine-learning-models)"
      ]
    }
  ]
}