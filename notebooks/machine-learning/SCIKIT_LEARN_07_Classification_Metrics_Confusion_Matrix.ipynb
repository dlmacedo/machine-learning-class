{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "SCIKIT_LEARN_07_Classification_Metrics_Confusion_Matrix.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dlmacedo/maxtrack/blob/master/notebooks/machine-learning/SCIKIT_LEARN_07_Classification_Metrics_Confusion_Matrix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVP4mX7zbOm5",
        "colab_type": "text"
      },
      "source": [
        "# Evaluating a classification model: Confusion matrix\n",
        "\n",
        "Created by [Data School](http://www.dataschool.io/). Modified by [David MacÃªdo](https://github.com/dlmacedo). **Note:** This notebook uses Python 3.6 and scikit-learn 0.19.1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2cJpnpzbOm8",
        "colab_type": "text"
      },
      "source": [
        "## Agenda\n",
        "\n",
        "- How does a **confusion matrix** describe the performance of a classifier?\n",
        "- What **metrics** can be computed from a confusion matrix?\n",
        "- How can you adjust classifier performance by **changing the classification threshold**?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpGwVpmLbOm9",
        "colab_type": "text"
      },
      "source": [
        "## Review of model evaluation\n",
        "\n",
        "- Need a way to choose between models: different model types, tuning parameters, and features\n",
        "- Use a **model evaluation procedure** to estimate how well a model will generalize to out-of-sample data\n",
        "- Requires a **model evaluation metric** to quantify the model performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2ppIyM4bOm-",
        "colab_type": "text"
      },
      "source": [
        "### Model evaluation procedures\n",
        "\n",
        "1. **Training and testing on the same data**\n",
        "    - Rewards overly complex models that \"overfit\" the training data and won't necessarily generalize\n",
        "2. **Train/test split**\n",
        "    - Split the dataset into two pieces, so that the model can be trained and tested on different data\n",
        "    - Better estimate of out-of-sample performance, but still a \"high variance\" estimate\n",
        "    - Useful due to its speed, simplicity, and flexibility\n",
        "3. **K-fold cross-validation**\n",
        "    - Systematically create \"K\" train/test splits and average the results together\n",
        "    - Even better estimate of out-of-sample performance\n",
        "    - Runs \"K\" times slower than train/test split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvjhEtJobOnf",
        "colab_type": "text"
      },
      "source": [
        "## Confusion matrix\n",
        "\n",
        "Table that describes the performance of a classification model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdItD0F5bOnf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# IMPORTANT: first argument is true values, second argument is predicted values\n",
        "print(metrics.confusion_matrix(y_test, y_pred_class))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J52HZhoWbOnj",
        "colab_type": "text"
      },
      "source": [
        "![Small confusion matrix](https://github.com/justmarkham/scikit-learn-videos/blob/master/images/09_confusion_matrix_1.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvmQYzfRbOnk",
        "colab_type": "text"
      },
      "source": [
        "- Every observation in the testing set is represented in **exactly one box**\n",
        "- It's a 2x2 matrix because there are **2 response classes**\n",
        "- The format shown here is **not** universal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xu7qmxw6bOnk",
        "colab_type": "text"
      },
      "source": [
        "**Basic terminology**\n",
        "\n",
        "- **True Positives (TP):** we *correctly* predicted that they *do* have diabetes\n",
        "- **True Negatives (TN):** we *correctly* predicted that they *don't* have diabetes\n",
        "- **False Positives (FP):** we *incorrectly* predicted that they *do* have diabetes (a \"Type I error\")\n",
        "- **False Negatives (FN):** we *incorrectly* predicted that they *don't* have diabetes (a \"Type II error\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YDwKevXbOnl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print the first 25 true and predicted responses\n",
        "print('True:', y_test.values[0:25])\n",
        "print('Pred:', y_pred_class[0:25])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdTlkGVGbOnm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save confusion matrix and slice into four pieces\n",
        "confusion = metrics.confusion_matrix(y_test, y_pred_class)\n",
        "TP = confusion[1, 1]\n",
        "TN = confusion[0, 0]\n",
        "FP = confusion[0, 1]\n",
        "FN = confusion[1, 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaLCc5s-bOnq",
        "colab_type": "text"
      },
      "source": [
        "![Large confusion matrix](https://github.com/justmarkham/scikit-learn-videos/blob/master/images/09_confusion_matrix_2.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2n3sD2JKbOnq",
        "colab_type": "text"
      },
      "source": [
        "## Metrics computed from a confusion matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Sdrz8yCbOnr",
        "colab_type": "text"
      },
      "source": [
        "**Classification Accuracy:** Overall, how often is the classifier correct?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsBzR7R9bOns",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print((TP + TN) / float(TP + TN + FP + FN))\n",
        "print(metrics.accuracy_score(y_test, y_pred_class))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2MFiowObOnt",
        "colab_type": "text"
      },
      "source": [
        "**Classification Error:** Overall, how often is the classifier incorrect?\n",
        "\n",
        "- Also known as \"Misclassification Rate\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luDzq_DqbOnu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print((FP + FN) / float(TP + TN + FP + FN))\n",
        "print(1 - metrics.accuracy_score(y_test, y_pred_class))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fvz27hidbOnv",
        "colab_type": "text"
      },
      "source": [
        "**Sensitivity:** When the actual value is positive, how often is the prediction correct?\n",
        "\n",
        "- How \"sensitive\" is the classifier to detecting positive instances?\n",
        "- Also known as \"True Positive Rate\" or \"Recall\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5Oiw-MCbOnw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(TP / float(TP + FN))\n",
        "print(metrics.recall_score(y_test, y_pred_class))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSW_Yhk6bOnx",
        "colab_type": "text"
      },
      "source": [
        "**Specificity:** When the actual value is negative, how often is the prediction correct?\n",
        "\n",
        "- How \"specific\" (or \"selective\") is the classifier in predicting positive instances?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgpwmJf2bOny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(TN / float(TN + FP))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mZyJGdsbOnz",
        "colab_type": "text"
      },
      "source": [
        "**False Positive Rate:** When the actual value is negative, how often is the prediction incorrect?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3vwj0CNbOn0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(FP / float(TN + FP))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8LvTLnCbOn1",
        "colab_type": "text"
      },
      "source": [
        "**Precision:** When a positive value is predicted, how often is the prediction correct?\n",
        "\n",
        "- How \"precise\" is the classifier when predicting positive instances?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eu6f_g9zbOn2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(TP / float(TP + FP))\n",
        "print(metrics.precision_score(y_test, y_pred_class))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkI4hojlbOn3",
        "colab_type": "text"
      },
      "source": [
        "Many other metrics can be computed: F1 score, Matthews correlation coefficient, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OkJLHsxbOn4",
        "colab_type": "text"
      },
      "source": [
        "**Conclusion:**\n",
        "\n",
        "- Confusion matrix gives you a **more complete picture** of how your classifier is performing\n",
        "- Also allows you to compute various **classification metrics**, and these metrics can guide your model selection\n",
        "\n",
        "**Which metrics should you focus on?**\n",
        "\n",
        "- Choice of metric depends on your **business objective**\n",
        "- **Spam filter** (positive class is \"spam\"): Optimize for **precision or specificity** because false negatives (spam goes to the inbox) are more acceptable than false positives (non-spam is caught by the spam filter)\n",
        "- **Fraudulent transaction detector** (positive class is \"fraud\"): Optimize for **sensitivity** because false positives (normal transactions that are flagged as possible fraud) are more acceptable than false negatives (fraudulent transactions that are not detected)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMfEyM2cbOn4",
        "colab_type": "text"
      },
      "source": [
        "## Adjusting the classification threshold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YRMMSWobOn5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print the first 10 predicted responses\n",
        "logreg.predict(X_test)[0:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7eSd0labOn7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print the first 10 predicted probabilities of class membership\n",
        "logreg.predict_proba(X_test)[0:10, :]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTzCdNvYbOn9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print the first 10 predicted probabilities for class 1\n",
        "logreg.predict_proba(X_test)[0:10, 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PvF3EbZbOn_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# store the predicted probabilities for class 1\n",
        "y_pred_prob = logreg.predict_proba(X_test)[:, 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQH5Q4O9bOoB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# allow plots to appear in the notebook\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIzY1Ac9bOoC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# histogram of predicted probabilities\n",
        "plt.hist(y_pred_prob, bins=8)\n",
        "plt.xlim(0, 1)\n",
        "plt.title('Histogram of predicted probabilities')\n",
        "plt.xlabel('Predicted probability of diabetes')\n",
        "plt.ylabel('Frequency')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmRv43rqbOoE",
        "colab_type": "text"
      },
      "source": [
        "**Decrease the threshold** for predicting diabetes in order to **increase the sensitivity** of the classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNVykvd2bOoE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# predict diabetes if the predicted probability is greater than 0.3\n",
        "from sklearn.preprocessing import binarize\n",
        "y_pred_class = binarize([y_pred_prob], 0.3)[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpUUBUmKbOoG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print the first 10 predicted probabilities\n",
        "y_pred_prob[0:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0MSHc3ibOoH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print the first 10 predicted classes with the lower threshold\n",
        "y_pred_class[0:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0XPnu6MbOoJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# previous confusion matrix (default threshold of 0.5)\n",
        "print(confusion)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqPYY4eqbOoK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# new confusion matrix (threshold of 0.3)\n",
        "print(metrics.confusion_matrix(y_test, y_pred_class))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szY8Vd2YbOoM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sensitivity has increased (used to be 0.24)\n",
        "print(46 / float(46 + 16))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCoO43a0bOoN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# specificity has decreased (used to be 0.91)\n",
        "print(80 / float(80 + 50))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLmMdaAAbOoQ",
        "colab_type": "text"
      },
      "source": [
        "**Conclusion:**\n",
        "\n",
        "- **Threshold of 0.5** is used by default (for binary problems) to convert predicted probabilities into class predictions\n",
        "- Threshold can be **adjusted** to increase sensitivity or specificity\n",
        "- Sensitivity and specificity have an **inverse relationship**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a_nxH55bOoe",
        "colab_type": "text"
      },
      "source": [
        "## Confusion Matrix Resources\n",
        "\n",
        "- Blog post: [Simple guide to confusion matrix terminology](http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/)\n",
        "- Videos: [Intuitive sensitivity and specificity](https://www.youtube.com/watch?v=U4_3fditnWg) (9 minutes) and [The tradeoff between sensitivity and specificity](https://www.youtube.com/watch?v=vtYDyGGeQyo) (13 minutes) by Rahul Patwari\n",
        "- Notebook: [How to calculate \"expected value\"](https://github.com/podopie/DAT18NYC/blob/master/classes/13-expected_value_cost_benefit_analysis.ipynb) from a confusion matrix by treating it as a cost-benefit matrix (by Ed Podojil)\n",
        "- Graphic: How [classification threshold](https://media.amazonwebservices.com/blog/2015/ml_adjust_model_1.png) affects different evaluation metrics (from a [blog post](https://aws.amazon.com/blogs/aws/amazon-machine-learning-make-data-driven-decisions-at-scale/) about Amazon Machine Learning)\n"
      ]
    }
  ]
}