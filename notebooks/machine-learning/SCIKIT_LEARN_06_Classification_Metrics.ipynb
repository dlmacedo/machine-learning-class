{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "SCIKIT_LEARN_06_Classification_Metrics.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dlmacedo/maxtrack/blob/master/notebooks/machine-learning/SCIKIT_LEARN_06_Classification_Metrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVP4mX7zbOm5",
        "colab_type": "text"
      },
      "source": [
        "# Evaluating a classification model\n",
        "\n",
        "Created by [Data School](http://www.dataschool.io/). Modified by [David MacÃªdo](https://github.com/dlmacedo). **Note:** This notebook uses Python 3.6 and scikit-learn 0.19.1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2cJpnpzbOm8",
        "colab_type": "text"
      },
      "source": [
        "## Agenda\n",
        "\n",
        "- What is the purpose of **model evaluation**, and what are some common evaluation procedures?\n",
        "- What is the usage of **classification accuracy**, and what are its limitations?\n",
        "- How does a **confusion matrix** describe the performance of a classifier?\n",
        "- What **metrics** can be computed from a confusion matrix?\n",
        "- How can you adjust classifier performance by **changing the classification threshold**?\n",
        "- What is the purpose of an **ROC curve**?\n",
        "- How does **Area Under the Curve (AUC)** differ from classification accuracy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpGwVpmLbOm9",
        "colab_type": "text"
      },
      "source": [
        "## Review of model evaluation\n",
        "\n",
        "- Need a way to choose between models: different model types, tuning parameters, and features\n",
        "- Use a **model evaluation procedure** to estimate how well a model will generalize to out-of-sample data\n",
        "- Requires a **model evaluation metric** to quantify the model performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2ppIyM4bOm-",
        "colab_type": "text"
      },
      "source": [
        "### Model evaluation procedures\n",
        "\n",
        "1. **Training and testing on the same data**\n",
        "    - Rewards overly complex models that \"overfit\" the training data and won't necessarily generalize\n",
        "2. **Train/test split**\n",
        "    - Split the dataset into two pieces, so that the model can be trained and tested on different data\n",
        "    - Better estimate of out-of-sample performance, but still a \"high variance\" estimate\n",
        "    - Useful due to its speed, simplicity, and flexibility\n",
        "3. **K-fold cross-validation**\n",
        "    - Systematically create \"K\" train/test splits and average the results together\n",
        "    - Even better estimate of out-of-sample performance\n",
        "    - Runs \"K\" times slower than train/test split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccF4G3xBbOm_",
        "colab_type": "text"
      },
      "source": [
        "### Model evaluation metrics\n",
        "\n",
        "- **Regression problems:** Mean Absolute Error, Mean Squared Error, Root Mean Squared Error\n",
        "- **Classification problems:** Classification accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nef5_zSAbOnA",
        "colab_type": "text"
      },
      "source": [
        "## Classification accuracy\n",
        "\n",
        "[Pima Indians Diabetes dataset](https://www.kaggle.com/uciml/pima-indians-diabetes-database) originally from the UCI Machine Learning Repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp0f1HLzbOnB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read the data into a pandas DataFrame\n",
        "import pandas as pd\n",
        "path = 'https://raw.githubusercontent.com/justmarkham/scikit-learn-videos/master/data/pima-indians-diabetes.data'\n",
        "col_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label']\n",
        "pima = pd.read_csv(path, header=None, names=col_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgKtJ5FUbOnE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print the first 5 rows of data\n",
        "pima.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMN6JJBJbOnH",
        "colab_type": "text"
      },
      "source": [
        "**Question:** Can we predict the diabetes status of a patient given their health measurements?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQJpq6IQbOnI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define X and y\n",
        "feature_cols = ['pregnant', 'insulin', 'bmi', 'age']\n",
        "X = pima[feature_cols]\n",
        "y = pima.label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pstvlbN-bOnK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split X and y into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VdOe83_bOnL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train a logistic regression model on the training set\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logreg = LogisticRegression()\n",
        "logreg.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNVK31nWbOnN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make class predictions for the testing set\n",
        "y_pred_class = logreg.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDJocWpMbOnP",
        "colab_type": "text"
      },
      "source": [
        "**Classification accuracy:** percentage of correct predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8ieUTnmbOnQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate accuracy\n",
        "from sklearn import metrics\n",
        "print(metrics.accuracy_score(y_test, y_pred_class))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ki7osvRbOnS",
        "colab_type": "text"
      },
      "source": [
        "**Null accuracy:** accuracy that could be achieved by always predicting the most frequent class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNgMxCnGbOnS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# examine the class distribution of the testing set (using a Pandas Series method)\n",
        "y_test.value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1uyWD59bOnU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate the percentage of ones\n",
        "y_test.mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkVlTX6sbOnW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate the percentage of zeros\n",
        "1 - y_test.mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pk5SEfe4bOnY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate null accuracy (for binary classification problems coded as 0/1)\n",
        "max(y_test.mean(), 1 - y_test.mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9E_iSswDbOna",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate null accuracy (for multi-class classification problems)\n",
        "y_test.value_counts().head(1) / len(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0ApUdE8bOnc",
        "colab_type": "text"
      },
      "source": [
        "Comparing the **true** and **predicted** response values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngC11r5IbOnc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print the first 25 true and predicted responses\n",
        "print('True:', y_test.values[0:25])\n",
        "print('Pred:', y_pred_class[0:25])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClHliTZ1bOne",
        "colab_type": "text"
      },
      "source": [
        "**Conclusion:**\n",
        "\n",
        "- Classification accuracy is the **easiest classification metric to understand**\n",
        "- But, it does not tell you the **underlying distribution** of response values\n",
        "- And, it does not tell you what **\"types\" of errors** your classifier is making"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvjhEtJobOnf",
        "colab_type": "text"
      },
      "source": [
        "## Confusion matrix\n",
        "\n",
        "Table that describes the performance of a classification model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdItD0F5bOnf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# IMPORTANT: first argument is true values, second argument is predicted values\n",
        "print(metrics.confusion_matrix(y_test, y_pred_class))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J52HZhoWbOnj",
        "colab_type": "text"
      },
      "source": [
        "![Small confusion matrix](https://github.com/justmarkham/scikit-learn-videos/blob/master/images/09_confusion_matrix_1.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvmQYzfRbOnk",
        "colab_type": "text"
      },
      "source": [
        "- Every observation in the testing set is represented in **exactly one box**\n",
        "- It's a 2x2 matrix because there are **2 response classes**\n",
        "- The format shown here is **not** universal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xu7qmxw6bOnk",
        "colab_type": "text"
      },
      "source": [
        "**Basic terminology**\n",
        "\n",
        "- **True Positives (TP):** we *correctly* predicted that they *do* have diabetes\n",
        "- **True Negatives (TN):** we *correctly* predicted that they *don't* have diabetes\n",
        "- **False Positives (FP):** we *incorrectly* predicted that they *do* have diabetes (a \"Type I error\")\n",
        "- **False Negatives (FN):** we *incorrectly* predicted that they *don't* have diabetes (a \"Type II error\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YDwKevXbOnl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print the first 25 true and predicted responses\n",
        "print('True:', y_test.values[0:25])\n",
        "print('Pred:', y_pred_class[0:25])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdTlkGVGbOnm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save confusion matrix and slice into four pieces\n",
        "confusion = metrics.confusion_matrix(y_test, y_pred_class)\n",
        "TP = confusion[1, 1]\n",
        "TN = confusion[0, 0]\n",
        "FP = confusion[0, 1]\n",
        "FN = confusion[1, 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaLCc5s-bOnq",
        "colab_type": "text"
      },
      "source": [
        "![Large confusion matrix](https://github.com/justmarkham/scikit-learn-videos/blob/master/images/09_confusion_matrix_2.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2n3sD2JKbOnq",
        "colab_type": "text"
      },
      "source": [
        "## Metrics computed from a confusion matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Sdrz8yCbOnr",
        "colab_type": "text"
      },
      "source": [
        "**Classification Accuracy:** Overall, how often is the classifier correct?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsBzR7R9bOns",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print((TP + TN) / float(TP + TN + FP + FN))\n",
        "print(metrics.accuracy_score(y_test, y_pred_class))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2MFiowObOnt",
        "colab_type": "text"
      },
      "source": [
        "**Classification Error:** Overall, how often is the classifier incorrect?\n",
        "\n",
        "- Also known as \"Misclassification Rate\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luDzq_DqbOnu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print((FP + FN) / float(TP + TN + FP + FN))\n",
        "print(1 - metrics.accuracy_score(y_test, y_pred_class))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fvz27hidbOnv",
        "colab_type": "text"
      },
      "source": [
        "**Sensitivity:** When the actual value is positive, how often is the prediction correct?\n",
        "\n",
        "- How \"sensitive\" is the classifier to detecting positive instances?\n",
        "- Also known as \"True Positive Rate\" or \"Recall\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5Oiw-MCbOnw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(TP / float(TP + FN))\n",
        "print(metrics.recall_score(y_test, y_pred_class))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSW_Yhk6bOnx",
        "colab_type": "text"
      },
      "source": [
        "**Specificity:** When the actual value is negative, how often is the prediction correct?\n",
        "\n",
        "- How \"specific\" (or \"selective\") is the classifier in predicting positive instances?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgpwmJf2bOny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(TN / float(TN + FP))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mZyJGdsbOnz",
        "colab_type": "text"
      },
      "source": [
        "**False Positive Rate:** When the actual value is negative, how often is the prediction incorrect?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3vwj0CNbOn0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(FP / float(TN + FP))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8LvTLnCbOn1",
        "colab_type": "text"
      },
      "source": [
        "**Precision:** When a positive value is predicted, how often is the prediction correct?\n",
        "\n",
        "- How \"precise\" is the classifier when predicting positive instances?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eu6f_g9zbOn2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(TP / float(TP + FP))\n",
        "print(metrics.precision_score(y_test, y_pred_class))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkI4hojlbOn3",
        "colab_type": "text"
      },
      "source": [
        "Many other metrics can be computed: F1 score, Matthews correlation coefficient, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OkJLHsxbOn4",
        "colab_type": "text"
      },
      "source": [
        "**Conclusion:**\n",
        "\n",
        "- Confusion matrix gives you a **more complete picture** of how your classifier is performing\n",
        "- Also allows you to compute various **classification metrics**, and these metrics can guide your model selection\n",
        "\n",
        "**Which metrics should you focus on?**\n",
        "\n",
        "- Choice of metric depends on your **business objective**\n",
        "- **Spam filter** (positive class is \"spam\"): Optimize for **precision or specificity** because false negatives (spam goes to the inbox) are more acceptable than false positives (non-spam is caught by the spam filter)\n",
        "- **Fraudulent transaction detector** (positive class is \"fraud\"): Optimize for **sensitivity** because false positives (normal transactions that are flagged as possible fraud) are more acceptable than false negatives (fraudulent transactions that are not detected)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMfEyM2cbOn4",
        "colab_type": "text"
      },
      "source": [
        "## Adjusting the classification threshold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YRMMSWobOn5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print the first 10 predicted responses\n",
        "logreg.predict(X_test)[0:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7eSd0labOn7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print the first 10 predicted probabilities of class membership\n",
        "logreg.predict_proba(X_test)[0:10, :]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTzCdNvYbOn9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print the first 10 predicted probabilities for class 1\n",
        "logreg.predict_proba(X_test)[0:10, 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PvF3EbZbOn_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# store the predicted probabilities for class 1\n",
        "y_pred_prob = logreg.predict_proba(X_test)[:, 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQH5Q4O9bOoB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# allow plots to appear in the notebook\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIzY1Ac9bOoC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# histogram of predicted probabilities\n",
        "plt.hist(y_pred_prob, bins=8)\n",
        "plt.xlim(0, 1)\n",
        "plt.title('Histogram of predicted probabilities')\n",
        "plt.xlabel('Predicted probability of diabetes')\n",
        "plt.ylabel('Frequency')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmRv43rqbOoE",
        "colab_type": "text"
      },
      "source": [
        "**Decrease the threshold** for predicting diabetes in order to **increase the sensitivity** of the classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNVykvd2bOoE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# predict diabetes if the predicted probability is greater than 0.3\n",
        "from sklearn.preprocessing import binarize\n",
        "y_pred_class = binarize([y_pred_prob], 0.3)[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpUUBUmKbOoG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print the first 10 predicted probabilities\n",
        "y_pred_prob[0:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0MSHc3ibOoH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print the first 10 predicted classes with the lower threshold\n",
        "y_pred_class[0:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0XPnu6MbOoJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# previous confusion matrix (default threshold of 0.5)\n",
        "print(confusion)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqPYY4eqbOoK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# new confusion matrix (threshold of 0.3)\n",
        "print(metrics.confusion_matrix(y_test, y_pred_class))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szY8Vd2YbOoM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sensitivity has increased (used to be 0.24)\n",
        "print(46 / float(46 + 16))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCoO43a0bOoN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# specificity has decreased (used to be 0.91)\n",
        "print(80 / float(80 + 50))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLmMdaAAbOoQ",
        "colab_type": "text"
      },
      "source": [
        "**Conclusion:**\n",
        "\n",
        "- **Threshold of 0.5** is used by default (for binary problems) to convert predicted probabilities into class predictions\n",
        "- Threshold can be **adjusted** to increase sensitivity or specificity\n",
        "- Sensitivity and specificity have an **inverse relationship**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ys6zzHBDbOoQ",
        "colab_type": "text"
      },
      "source": [
        "## ROC Curves and Area Under the Curve (AUC)\n",
        "\n",
        "**Question:** Wouldn't it be nice if we could see how sensitivity and specificity are affected by various thresholds, without actually changing the threshold?\n",
        "\n",
        "**Answer:** Plot the ROC curve!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fk2Lu7irbOoQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# IMPORTANT: first argument is true values, second argument is predicted probabilities\n",
        "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_prob)\n",
        "plt.plot(fpr, tpr)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.0])\n",
        "plt.title('ROC curve for diabetes classifier')\n",
        "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
        "plt.ylabel('True Positive Rate (Sensitivity)')\n",
        "plt.grid(True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQB4wPWWbOoT",
        "colab_type": "text"
      },
      "source": [
        "- ROC curve can help you to **choose a threshold** that balances sensitivity and specificity in a way that makes sense for your particular context\n",
        "- You can't actually **see the thresholds** used to generate the curve on the ROC curve itself"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a78l7Zn2bOoT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define a function that accepts a threshold and prints sensitivity and specificity\n",
        "def evaluate_threshold(threshold):\n",
        "    print('Sensitivity:', tpr[thresholds > threshold][-1])\n",
        "    print('Specificity:', 1 - fpr[thresholds > threshold][-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlBpzOldbOoV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluate_threshold(0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFlEUwlxbOoW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluate_threshold(0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lw8nrVXBbOoX",
        "colab_type": "text"
      },
      "source": [
        "AUC is the **percentage** of the ROC plot that is **underneath the curve**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYhaqUKPbOoY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# IMPORTANT: first argument is true values, second argument is predicted probabilities\n",
        "print(metrics.roc_auc_score(y_test, y_pred_prob))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TeewbglbOoa",
        "colab_type": "text"
      },
      "source": [
        "- AUC is useful as a **single number summary** of classifier performance.\n",
        "- If you randomly chose one positive and one negative observation, AUC represents the likelihood that your classifier will assign a **higher predicted probability** to the positive observation.\n",
        "- AUC is useful even when there is **high class imbalance** (unlike classification accuracy)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RFhfFjYbOob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate cross-validated AUC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "cross_val_score(logreg, X, y, cv=10, scoring='roc_auc').mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGKNMrBJbOoe",
        "colab_type": "text"
      },
      "source": [
        "**Confusion matrix advantages:**\n",
        "\n",
        "- Allows you to calculate a **variety of metrics**\n",
        "- Useful for **multi-class problems** (more than two response classes)\n",
        "\n",
        "**ROC/AUC advantages:**\n",
        "\n",
        "- Does not require you to **set a classification threshold**\n",
        "- Still useful when there is **high class imbalance**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a_nxH55bOoe",
        "colab_type": "text"
      },
      "source": [
        "## Confusion Matrix Resources\n",
        "\n",
        "- Blog post: [Simple guide to confusion matrix terminology](http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/)\n",
        "- Videos: [Intuitive sensitivity and specificity](https://www.youtube.com/watch?v=U4_3fditnWg) (9 minutes) and [The tradeoff between sensitivity and specificity](https://www.youtube.com/watch?v=vtYDyGGeQyo) (13 minutes) by Rahul Patwari\n",
        "- Notebook: [How to calculate \"expected value\"](https://github.com/podopie/DAT18NYC/blob/master/classes/13-expected_value_cost_benefit_analysis.ipynb) from a confusion matrix by treating it as a cost-benefit matrix (by Ed Podojil)\n",
        "- Graphic: How [classification threshold](https://media.amazonwebservices.com/blog/2015/ml_adjust_model_1.png) affects different evaluation metrics (from a [blog post](https://aws.amazon.com/blogs/aws/amazon-machine-learning-make-data-driven-decisions-at-scale/) about Amazon Machine Learning)\n",
        "\n",
        "\n",
        "## ROC and AUC Resources\n",
        "\n",
        "- Video: [ROC Curves and Area Under the Curve](https://www.youtube.com/watch?v=OAl6eAyP-yo) (14 minutes), including [transcript and screenshots](http://www.dataschool.io/roc-curves-and-auc-explained/) and a [visualization](http://www.navan.name/roc/)\n",
        "- Video: [ROC Curves](https://www.youtube.com/watch?v=21Igj5Pr6u4) (12 minutes) by Rahul Patwari\n",
        "- Paper: [An introduction to ROC analysis](http://people.inf.elte.hu/kiss/13dwhdm/roc.pdf) by Tom Fawcett\n",
        "- Usage examples: [Comparing different feature sets](http://research.microsoft.com/pubs/205472/aisec10-leontjeva.pdf) for detecting fraudulent Skype users, and [comparing different classifiers](http://www.cse.ust.hk/nevinZhangGroup/readings/yi/Bradley_PR97.pdf) on a number of popular datasets\n",
        "\n",
        "## Other Resources\n",
        "\n",
        "- scikit-learn documentation: [Model evaluation](http://scikit-learn.org/stable/modules/model_evaluation.html)\n",
        "- Guide: [Comparing model evaluation procedures and metrics](https://github.com/justmarkham/DAT8/blob/master/other/model_evaluation_comparison.md)\n",
        "- Video: [Counterfactual evaluation of machine learning models](https://www.youtube.com/watch?v=QWCSxAKR-h0) (45 minutes) about how Stripe evaluates its fraud detection model, including [slides](http://www.slideshare.net/MichaelManapat/counterfactual-evaluation-of-machine-learning-models)"
      ]
    }
  ]
}